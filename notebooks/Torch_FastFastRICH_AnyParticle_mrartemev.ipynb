{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARTICLE_TYPE = 'pion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"FastFastRICH_Cramer_{}_5layers\".format(PARTICLE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import scipy\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from torch.autograd import Variable, grad\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich_utils.torch_utils_rich_mrartemev as utils_rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and concatenating datasets:\n",
      "\t.././RichGAN/data_calibsample/pion2_+_down_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion2_-_up_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion_+_up_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion_+_down_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion2_+_up_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion_-_up_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion_-_down_2016_.csv\n",
      "\t.././RichGAN/data_calibsample/pion2_-_down_2016_.csv\n",
      "splitting to train/val/test\n",
      "fitting the scaler\n",
      "scaler train sample size: 2000000\n",
      "scaler n_quantiles: 100000, time = 1.7589631080627441\n",
      "scaling train set\n",
      "scaling test set\n",
      "converting dtype to <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, scaler = utils_rich.get_merged_typed_dataset(PARTICLE_TYPE, dtype=np.float32, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = int(1e3)\n",
    "LATENT_DIMENSIONS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import Tensor\n",
    "\n",
    "def make_data_loader(data_train):\n",
    "    all_data = TensorDataset(Tensor(data_train.values[:,:-1]), Tensor(data_train.values[:,-1]), \n",
    "                             Tensor((data_train.values[:, utils_rich.y_count:])[:,:-1]), \n",
    "                             Tensor((data_train.values[:, utils_rich.y_count:])[:,-1]),\n",
    "                             Tensor((data_train.values[:, utils_rich.y_count:])[:,:-1]), \n",
    "                             Tensor((data_train.values[:, utils_rich.y_count:])[:,-1]))\n",
    "    return DataLoader(all_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataloader = make_data_loader(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRAMER_DIM = 256\n",
    "NUM_LAYERS = 5\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear((LATENT_DIMENSIONS + data_train.shape[1] - 1 - utils_rich.y_count), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, utils_rich.y_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        noise = torch.empty(input.shape[0], LATENT_DIMENSIONS, device=input.device).normal_(mean=0,std=3.0)\n",
    "        return self.main(torch.cat((noise, input), dim=1))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear((data_train.shape[1] - 1), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, CRAMER_DIM)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "netG = Generator().to(device)\n",
    "netC = Critic().to(device)\n",
    "netC.apply(init_weights)\n",
    "netG.apply(init_weights)\n",
    "print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optC = torch.optim.RMSprop(netC.parameters(), lr=1e-3)\n",
    "lr_C = torch.optim.lr_scheduler.ExponentialLR(optimizer=optC, gamma=0.98)\n",
    "optG = torch.optim.RMSprop(netG.parameters(), lr=1e-3)\n",
    "lr_G = torch.optim.lr_scheduler.ExponentialLR(optimizer=optG, gamma=0.98)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = \"./log\"\n",
    "CRITIC_ITERATIONS_CONST = 15\n",
    "CRITIC_ITERATIONS_VAR = 0\n",
    "TOTAL_ITERATIONS = int(1e5)\n",
    "VALIDATION_INTERVAL = 1000\n",
    "critic_policy = lambda i: (\n",
    "    CRITIC_ITERATIONS_CONST + (CRITIC_ITERATIONS_VAR * (TOTAL_ITERATIONS - i)) // TOTAL_ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_critic(x, y):\n",
    "    discriminated_x = netC(x)\n",
    "    return torch.norm(discriminated_x - netC(y), dim=1) - torch.norm(discriminated_x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_pt = lambda i: 20 / np.pi * 2 * torch.atan(torch.tensor(i, dtype=torch.float32, device=device)/1e4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    N_VAL = int(3e5)\n",
    "    validation_np = data_val.sample(N_VAL).values\n",
    "    val = torch.tensor(validation_np, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/nzinci/general/26b1d03df60040f2af6d8090077853aa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"k62CZajG08ctPlNYUYRv9YVdO\",\n",
    "                        project_name=\"general\", workspace=\"nzinci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(TOTAL_ITERATIONS), position=0, leave=True):\n",
    "    for j in range(critic_policy(i)):\n",
    "        train_full, w_full, train_x_1, w_x_1, train_x_2, w_x_2 = next(iter(all_dataloader))\n",
    "        train_full = train_full.to(device)\n",
    "        w_full = w_full.to(device)\n",
    "        train_x_1 = train_x_1.to(device)\n",
    "        w_x_1 = w_x_1.to(device)\n",
    "        train_x_2 = train_x_2.to(device)\n",
    "        w_x_2 = w_x_2.to(device)\n",
    "\n",
    "        optC.zero_grad()\n",
    "        output = netC(train_full)\n",
    "        gen_y_1 = netG(train_x_1)\n",
    "        gen_y_2 = netG(train_x_2)\n",
    "        gen_full_2 = torch.cat((gen_y_2, train_x_2), dim=1)\n",
    "        gen_full_1 = torch.cat((gen_y_1, train_x_1), dim=1)\n",
    "        generator_loss = torch.mean(cramer_critic(train_full, gen_full_2) * w_full * w_x_2 -\n",
    "                    cramer_critic(gen_full_1, gen_full_2) * w_x_1  * w_x_2)\n",
    "\n",
    "        alpha = torch.empty(train_full.shape[0], 1, device=device).normal_(0.0,1.0)\n",
    "        interpolates = alpha * train_full + (1.0 - alpha) * gen_full_1\n",
    "        disc_interpolates = cramer_critic(interpolates, gen_full_2)\n",
    "        gradients = grad(outputs=disc_interpolates, inputs=interpolates, \n",
    "                         grad_outputs=torch.ones_like(disc_interpolates))[0]\n",
    "        slopes = torch.norm(torch.reshape(gradients, (list(gradients[0].shape)[0], -1)), dim=1)\n",
    "        gradient_penalty = torch.mean(torch.pow(torch.max(torch.abs(slopes) - 1, \n",
    "                                                          torch.zeros(8, device=device)), 2))\n",
    "        critic_loss = lambda_pt(i) * gradient_penalty - generator_loss\n",
    "        critic_loss.backward(retain_graph=True)\n",
    "        optC.step()\n",
    "    \n",
    "    train_full, w_full, train_x_1, w_x_1, train_x_2, w_x_2 = next(iter(all_dataloader))\n",
    "    train_full = train_full.to(device)\n",
    "    w_full = w_full.to(device)\n",
    "    train_x_1 = train_x_1.to(device)\n",
    "    w_x_1 = w_x_1.to(device)\n",
    "    train_x_2 = train_x_2.to(device)\n",
    "    w_x_2 = w_x_2.to(device)\n",
    "\n",
    "    optG.zero_grad()\n",
    "    generator_loss = torch.mean(cramer_critic(train_full, gen_full_2) * w_full * w_x_2 -\n",
    "                            cramer_critic(gen_full_1, gen_full_2) * w_x_1  * w_x_2)\n",
    "    generator_loss.backward()\n",
    "    optG.step()\n",
    "    experiment.log_metrics({'Generator loss': generator_loss.item(),\n",
    "                            'Critic loss': critic_loss.item()},\n",
    "                            step = i)\n",
    "    lr_C.step()\n",
    "    lr_G.step()\n",
    "    torch.save({'netC_state_dict': netC.state_dict(),\n",
    "                'netG_state_dict': netG.state_dict(),\n",
    "                'optC_state_dict': optC.state_dict(),\n",
    "                'optG_state_dict': optG.state_dict(),\n",
    "                'lr_C_state_dict': lr_C.state_dict(),\n",
    "                'lr_G_state_dict': lr_G.state_dict()\n",
    "               }, LOGDIR)\n",
    "\n",
    "    clear_output(False)\n",
    "    with torch.no_grad():\n",
    "        y_t = netG(val[:, utils_rich.y_count:-1])\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "        for INDEX, ax in zip((0, 1, 3, 4), axes.flatten()):\n",
    "            _, bins, _ = ax.hist(val[:, INDEX].cpu(), bins=100, label=\"data\", normed=True,\n",
    "                                 weights=val[:,-1].cpu())\n",
    "            ax.hist(y_t[:, INDEX].cpu(), bins=bins, label=\"generated\", alpha=0.5, normed=True,\n",
    "                    weights=val[:,-1].cpu())\n",
    "            ax.legend()\n",
    "            ax.set_title(utils_rich.dll_columns[INDEX])\n",
    "        experiment.log_figure()\n",
    "        plt.show()\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    y_t = netG(val[:, utils_rich.y_count:-1])\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    for INDEX, ax in zip((0, 1, 3, 4), axes.flatten()):\n",
    "        _, bins, _ = ax.hist(val[:, INDEX].cpu(), bins=100, label=\"data\", normed=True,\n",
    "                             weights=val[:,-1].cpu())\n",
    "        ax.hist(y_t[:, INDEX].cpu(), bins=bins, label=\"generated\", alpha=0.5, normed=True,\n",
    "                weights=val[:,-1].cpu())\n",
    "        ax.legend()\n",
    "        ax.set_title(utils_rich.dll_columns[INDEX])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
